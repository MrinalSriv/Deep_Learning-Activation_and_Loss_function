# Deep_Learning-Activation_and_Loss_function
 A Activation function in a neural network essential otherwise it is just a linear regression model. Although linear transformations make the neural network model simpler, but this network would be less powerful and will not be able to learn the complex patterns from data. There are so many activation functions available. Out of which few of them are 
               a) Sigmoid Function
               b) Tanh Function
               c)  Relu Function 
               d)  Leaky Relu Function
               e)  ELU Function
               f)	Softmax Function
               g)  PRELU Function
               h)  SWISS Function
               i)	Maxout Function
               j)	SoftPlus Function
